{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "2.1.2-tf\n",
      "Keras: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Own - Conda venv --- dc_info_venv\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "### main Source --- https://www.tensorflow.org/guide/\n",
    "\n",
    "# \n",
    "import tensorflow as tf\n",
    "#from tf.keras import layers ### Fails - We have TF version == 1.5.0 \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "#\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n",
    "import keras\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "\"\"\"\n",
    "A TFRecord file stores your data as a sequence of binary strings.\n",
    "This means you need to specify the structure of your data before you write it to the file.\n",
    "Tensorflow provides two components for this purpose: \n",
    "\n",
    "tf.train.Example and \n",
    "tf.train.SequenceExample. \n",
    "\n",
    "You have to store each sample of your data in one of these structures, \n",
    "then ----serialize-------- it and use a tf.python_io.TFRecordWriter to write it to disk.\n",
    "\"\"\"\n",
    "\n",
    "## DHANKAR --- FATT --- Some other sources mentinn getting IMAGES in as NUMPY ARRAYS ?\n",
    "## SOURCE ---- https://www.tensorflow.org/api_docs/python/tf/data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The tf.data API enables you to build complex input pipelines from simple, reusable pieces.\n",
    "For example, the pipeline for an image model might aggregate data from files in a ---- distributed file system,\n",
    "apply random perturbations to each image, and ------- merge randomly selected images ---- into a batch for training.\n",
    "\n",
    "The pipeline for a text model might involve extracting symbols from raw text data, converting\n",
    "them to embedding identifiers with a ----lookup table-----, and -----batching together sequences----\n",
    "of different lengths. \n",
    "\n",
    "The tf.data API makes it easy to deal with large amounts of data,\n",
    "different data formats, and complicated transformations.\n",
    "\"\"\"\n",
    "\n",
    "### tensor_1 == image_data\n",
    "### tensor_2 == image_label\n",
    "\"\"\"\n",
    "A tf.data.Dataset represents a sequence of elements, in which each element contains one or more ---Tensor-- objects.\n",
    "For example, in an--- image pipeline, an element might be a ----single training example---, with a pair of tensors\n",
    "representing the image data and a label.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset.from_tensor_slices()\n",
    "### Dataset.batch()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more tf.Tensor objects.\n",
    "\n",
    "    Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more tf.data.Dataset objects.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### tf.data.Iterator\n",
    "\n",
    "\"\"\"\n",
    "A tf.data.Iterator provides the main way to extract elements from a dataset. \n",
    "The operation returned by Iterator.get_next() yields the next element of a Dataset when executed,\n",
    "and typically acts as the interface between input pipeline code and your model.\n",
    "\n",
    "The simplest iterator is a \"one-shot iterator\", which is associated with a particular Dataset and \n",
    "iterates through it once.\n",
    "\n",
    "For more sophisticated uses, the Iterator.initializer operation enables you to reinitialize \n",
    "and parameterize an iterator with different datasets, \n",
    "so that you can, for example, \n",
    "iterate over training and validation data multiple times in the same program.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset structure\n",
    "# --- dataset >> elements >> tf.Tensor -- components >> tf.TensorShape\n",
    "\n",
    "\"\"\"\n",
    "Dataset structure\n",
    "\n",
    "A dataset comprises ---elements--- that each have the same structure. \n",
    "An element contains one or more ----tf.Tensor objects---, called ----components---.\n",
    "----- Each component has a tf.DType representing the type of elements in the tensor\n",
    "----- and a tf.TensorShape representing the (possibly partially specified) static shape of each element. \n",
    "\"\"\"\n",
    "\n",
    "### PROPERTIES ===>>  Dataset.output_types and Dataset.output_shapes\n",
    "\"\"\"\n",
    "The Dataset.output_types and Dataset.output_shapes properties \n",
    "\n",
    "----allow you to inspect the inferred types \n",
    "----and shapes of each component of a dataset element. \n",
    "\n",
    "The nested structure of these properties map to the structure of an element, \n",
    "--- which may be a single tensor, \n",
    "--- a tuple of tensors, \n",
    "--- or a nested tuple of tensors.\n",
    "\"\"\"\n",
    "\n",
    "### \n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(1000,)\n",
      "<TensorSliceDataset shapes: (1000,), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 1000]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "#\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "<TensorSliceDataset shapes: ((), (100,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "#\n",
    "print(dataset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is often convenient to give names to each component of an element, \n",
    "for example if they represent different features of a training example. \n",
    "\n",
    "In addition to tuples, you can use collections.namedtuple or a dictionary mapping strings to tensors \n",
    "to represent a single element of a Dataset.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.float32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([])}\n"
     ]
    }
   ],
   "source": [
    "### Official \n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.int32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([Dimension(500)])}\n"
     ]
    }
   ],
   "source": [
    "## DHANKAR ---\n",
    "\n",
    "dataset_11 = tf.data.Dataset.from_tensor_slices(\n",
    "   {\n",
    "    \"a\": tf.random_uniform([4, 500], maxval=1000, dtype=tf.int32),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    }\n",
    "      )\n",
    "\n",
    "print(dataset_11.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset_11.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.data' has no attribute 'CsvDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31dd95f298fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m   \u001b[0;31m# Eight required float columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.data' has no attribute 'CsvDataset'"
     ]
    }
   ],
   "source": [
    "### CSV Uploads --ERROR --- FATT \n",
    "# latest version of TF == has the CSV Func \n",
    "## Documentation for version --- 1.12 \n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset\n",
    "\n",
    "#v-1.6.0 --- Has Experimental --\n",
    "##- tensorflow/tensorflow/python/data/experimental/benchmarks/csv_dataset_benchmark.py\n",
    "\n",
    "\n",
    "# Right now using - v-1.5.0 --- which does not . \n",
    "# /a6_18/OwnFork_TensorFlow/tensorflow/tensorflow/contrib/data/python/ops/readers.py\n",
    "\n",
    "\n",
    "# Creates a dataset that reads all of the records from two CSV files, each with\n",
    "# eight float columns\n",
    "filenames = [\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"]\n",
    "\n",
    "record_defaults = [tf.float32] * 8   # Eight required float columns\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##FATT --- CSV OnHold\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "## https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/example/example.proto\n",
    "\n",
    "\"\"\"\n",
    "If your dataset consist of features, where each feature is a list of values of the same type, \n",
    "tf.train.Example is the right component to use.\n",
    "\n",
    "We have a number of features, \n",
    "each being a list where every entry has the same data type. \n",
    "In order to store these features in a TFRecord, \n",
    "we fist need to create the lists that constitute the features.\n",
    "\n",
    "tf.train.BytesList\n",
    "tf.train.FloatList\n",
    "tf.train.Int64List \n",
    "\n",
    "are at the core of a tf.train.Feature. \n",
    "\n",
    "All three have a single attribute value, which expects a list of respective \n",
    "--- bytes, \n",
    "--- float, \n",
    "--- int.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### tf.train.Feature\n",
    "\"\"\"\n",
    "tf.train.Feature ---  wraps a list of data of a specific type so Tensorflow can understand it.\n",
    "It has a single attribute, which is a ---union of ----bytes_list/float_list/int64_list. \n",
    "Being a union, the stored list can be of type \n",
    "--- tf.train.BytesList (attribute name bytes_list), \n",
    "--- tf.train.FloatList (attribute name float_list), \n",
    "--- tf.train.Int64List (attribute name int64_list).\n",
    "\n",
    "tf.train.Features ----PLURAL----Features---- is a collection of named features. \n",
    "It has a single attribute feature that expects a dictionary where the --- key ----is the name of the features \n",
    "---- and the value a tf.train.Feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In our example, each TFRecord represents the movie ratings and corresponding suggestions \n",
    "of a single user (a single sample). \n",
    "Writing recommendations for all users in the dataset follows the same process. \n",
    "It is important that the type of a feature (e.g. float for the movie rating) is the same across all samples \n",
    "in the dataset. \n",
    "This conformance criterion and others are defined in the protocol buffer definition of tf.train.Example.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': 29, 'Suggestion': 'Inception', 'Movie': ['The Shawshank Redemption', 'Fight Club'], 'Purchase Price': 9.99, 'Suggestion Purchased': 1.0, 'Movie Ratings': [9.0, 9.7]}\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "data = {\n",
    "    'Age': 29,\n",
    "    'Movie': ['The Shawshank Redemption', 'Fight Club'],\n",
    "    'Movie Ratings': [9.0, 9.7],\n",
    "    'Suggestion': 'Inception',\n",
    "    'Suggestion Purchased': 1.0,\n",
    "    'Purchase Price': 9.99\n",
    "}\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"Age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 29\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Movie\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"The Shawshank Redemption\"\n",
      "        value: \"Fight Club\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Movie Ratings\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 9.0\n",
      "        value: 9.699999809265137\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Purchase Price\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 9.989999771118164\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Suggestion\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"Inception\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Suggestion Purchased\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 1.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Example\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'Age': tf.train.Feature(\n",
    "        int64_list=tf.train.Int64List(value=[data['Age']])),\n",
    "    'Movie': tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[m.encode('utf-8') for m in data['Movie']])),\n",
    "    'Movie Ratings': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(value=data['Movie Ratings'])),\n",
    "    'Suggestion': tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[data['Suggestion'].encode('utf-8')])),\n",
    "    'Suggestion Purchased': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(\n",
    "            value=[data['Suggestion Purchased']])),\n",
    "    'Purchase Price': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(value=[data['Purchase Price']]))\n",
    "}))\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write TFrecord file\n",
    "with tf.python_io.TFRecordWriter('customer_1.tfrecord') as writer:\n",
    "    #\n",
    "    writer.write(example.SerializeToString())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 29\n",
      "Purchase Price: 9.989999771118164\n",
      "Movie: SparseTensorValue(indices=array([[0],\n",
      "       [1]]), values=array([b'The Shawshank Redemption', b'Fight Club'], dtype=object), dense_shape=array([2]))\n",
      "Suggestion Purchased: 1.0\n",
      "Movie Ratings: SparseTensorValue(indices=array([[0],\n",
      "       [1]]), values=array([9. , 9.7], dtype=float32), dense_shape=array([2]))\n",
      "Suggestion: b'Inception'\n"
     ]
    }
   ],
   "source": [
    "# Read and print data:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['customer_1.tfrecord'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "# Define features\n",
    "read_features = {\n",
    "    'Age': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'Movie': tf.VarLenFeature(dtype=tf.string),\n",
    "    'Movie Ratings': tf.VarLenFeature(dtype=tf.float32),\n",
    "    'Suggestion': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'Suggestion Purchased': tf.FixedLenFeature([], dtype=tf.float32),\n",
    "    'Purchase Price': tf.FixedLenFeature([], dtype=tf.float32)}\n",
    "\n",
    "# Extract features from serialized data\n",
    "read_data = tf.parse_single_example(serialized=serialized_example,\n",
    "                                    features=read_features)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "# Print features\n",
    "for name, tensor in read_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dc_info_venv]",
   "language": "python",
   "name": "conda-env-dc_info_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
