{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "2.1.2-tf\n",
      "Keras: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Own - Conda venv --- dc_info_venv\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "### main Source --- https://www.tensorflow.org/guide/\n",
    "\n",
    "# \n",
    "import tensorflow as tf\n",
    "#from tf.keras import layers ### Fails - We have TF version == 1.5.0 \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "#\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n",
    "import keras\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "\"\"\"\n",
    "A TFRecord file stores your data as a sequence of binary strings.\n",
    "This means you need to specify the structure of your data before you write it to the file.\n",
    "Tensorflow provides two components for this purpose: \n",
    "\n",
    "tf.train.Example and \n",
    "tf.train.SequenceExample. \n",
    "\n",
    "You have to store each sample of your data in one of these structures, \n",
    "then ----serialize-------- it and use a tf.python_io.TFRecordWriter to write it to disk.\n",
    "\"\"\"\n",
    "\n",
    "## DHANKAR --- FATT --- Some other sources mentinn getting IMAGES in as NUMPY ARRAYS ?\n",
    "## SOURCE ---- https://www.tensorflow.org/api_docs/python/tf/data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The tf.data API enables you to build complex input pipelines from simple, reusable pieces.\n",
    "For example, the pipeline for an image model might aggregate data from files in a ---- distributed file system,\n",
    "apply random perturbations to each image, and ------- merge randomly selected images ---- into a batch for training.\n",
    "\n",
    "The pipeline for a text model might involve extracting symbols from raw text data, converting\n",
    "them to embedding identifiers with a ----lookup table-----, and -----batching together sequences----\n",
    "of different lengths. \n",
    "\n",
    "The tf.data API makes it easy to deal with large amounts of data,\n",
    "different data formats, and complicated transformations.\n",
    "\"\"\"\n",
    "\n",
    "### tensor_1 == image_data\n",
    "### tensor_2 == image_label\n",
    "\"\"\"\n",
    "A tf.data.Dataset represents a sequence of elements, in which each element contains one or more ---Tensor-- objects.\n",
    "For example, in an--- image pipeline, an element might be a ----single training example---, with a pair of tensors\n",
    "representing the image data and a label.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset.from_tensor_slices()\n",
    "### Dataset.batch()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more tf.Tensor objects.\n",
    "\n",
    "    Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more tf.data.Dataset objects.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### tf.data.Iterator\n",
    "\n",
    "\"\"\"\n",
    "A tf.data.Iterator provides the main way to extract elements from a dataset. \n",
    "The operation returned by Iterator.get_next() yields the next element of a Dataset when executed,\n",
    "and typically acts as the interface between input pipeline code and your model.\n",
    "\n",
    "The simplest iterator is a \"one-shot iterator\", which is associated with a particular Dataset and \n",
    "iterates through it once.\n",
    "\n",
    "For more sophisticated uses, the Iterator.initializer operation enables you to reinitialize \n",
    "and parameterize an iterator with different datasets, \n",
    "so that you can, for example, \n",
    "iterate over training and validation data multiple times in the same program.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset structure\n",
    "# --- dataset >> elements >> tf.Tensor -- components >> tf.TensorShape\n",
    "\n",
    "\"\"\"\n",
    "Dataset structure\n",
    "\n",
    "A dataset comprises ---elements--- that each have the same structure. \n",
    "An element contains one or more ----tf.Tensor objects---, called ----components---.\n",
    "----- Each component has a tf.DType representing the type of elements in the tensor\n",
    "----- and a tf.TensorShape representing the (possibly partially specified) static shape of each element. \n",
    "\"\"\"\n",
    "\n",
    "### PROPERTIES ===>>  Dataset.output_types and Dataset.output_shapes\n",
    "\"\"\"\n",
    "The Dataset.output_types and Dataset.output_shapes properties \n",
    "\n",
    "----allow you to inspect the inferred types \n",
    "----and shapes of each component of a dataset element. \n",
    "\n",
    "The nested structure of these properties map to the structure of an element, \n",
    "--- which may be a single tensor, \n",
    "--- a tuple of tensors, \n",
    "--- or a nested tuple of tensors.\n",
    "\"\"\"\n",
    "\n",
    "### \n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(1000,)\n",
      "<TensorSliceDataset shapes: (1000,), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 1000]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "#\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "<TensorSliceDataset shapes: ((), (100,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "#\n",
    "print(dataset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is often convenient to give names to each component of an element, \n",
    "for example if they represent different features of a training example. \n",
    "\n",
    "In addition to tuples, you can use collections.namedtuple or a dictionary mapping strings to tensors \n",
    "to represent a single element of a Dataset.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.float32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([])}\n"
     ]
    }
   ],
   "source": [
    "### Official \n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.int32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([Dimension(500)])}\n"
     ]
    }
   ],
   "source": [
    "## DHANKAR ---\n",
    "\n",
    "dataset_11 = tf.data.Dataset.from_tensor_slices(\n",
    "   {\n",
    "    \"a\": tf.random_uniform([4, 500], maxval=1000, dtype=tf.int32),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    }\n",
    "      )\n",
    "\n",
    "print(dataset_11.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset_11.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.data' has no attribute 'CsvDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31dd95f298fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m   \u001b[0;31m# Eight required float columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.data' has no attribute 'CsvDataset'"
     ]
    }
   ],
   "source": [
    "### CSV Uploads --ERROR --- FATT \n",
    "# latest version of TF == has the CSV Func \n",
    "## Documentation for version --- 1.12 \n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset\n",
    "\n",
    "#v-1.6.0 --- Has Experimental --\n",
    "##- tensorflow/tensorflow/python/data/experimental/benchmarks/csv_dataset_benchmark.py\n",
    "\n",
    "\n",
    "# Right now using - v-1.5.0 --- which does not . \n",
    "# /a6_18/OwnFork_TensorFlow/tensorflow/tensorflow/contrib/data/python/ops/readers.py\n",
    "\n",
    "\n",
    "# Creates a dataset that reads all of the records from two CSV files, each with\n",
    "# eight float columns\n",
    "filenames = [\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"]\n",
    "\n",
    "record_defaults = [tf.float32] * 8   # Eight required float columns\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dc_info_venv]",
   "language": "python",
   "name": "conda-env-dc_info_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
