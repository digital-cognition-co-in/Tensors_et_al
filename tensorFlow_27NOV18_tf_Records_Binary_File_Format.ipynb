{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "2.1.2-tf\n",
      "Keras: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Own - Conda venv --- dc_info_venv\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "### main Source --- https://www.tensorflow.org/guide/\n",
    "\n",
    "# \n",
    "import tensorflow as tf\n",
    "#from tf.keras import layers ### Fails - We have TF version == 1.5.0 \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "#\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n",
    "import keras\n",
    "print('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "\"\"\"\n",
    "A TFRecord file stores your data as a sequence of binary strings.\n",
    "This means you need to specify the structure of your data before you write it to the file.\n",
    "Tensorflow provides two components for this purpose: \n",
    "\n",
    "tf.train.Example and \n",
    "tf.train.SequenceExample. \n",
    "\n",
    "You have to store each sample of your data in one of these structures, \n",
    "then ----serialize-------- it and use a tf.python_io.TFRecordWriter to write it to disk.\n",
    "\"\"\"\n",
    "\n",
    "## DHANKAR --- FATT --- Some other sources mentinn getting IMAGES in as NUMPY ARRAYS ?\n",
    "## SOURCE ---- https://www.tensorflow.org/api_docs/python/tf/data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The tf.data API enables you to build complex input pipelines from simple, reusable pieces.\n",
    "For example, the pipeline for an image model might aggregate data from files in a ---- distributed file system,\n",
    "apply random perturbations to each image, and ------- merge randomly selected images ---- into a batch for training.\n",
    "\n",
    "The pipeline for a text model might involve extracting symbols from raw text data, converting\n",
    "them to embedding identifiers with a ----lookup table-----, and -----batching together sequences----\n",
    "of different lengths. \n",
    "\n",
    "The tf.data API makes it easy to deal with large amounts of data,\n",
    "different data formats, and complicated transformations.\n",
    "\"\"\"\n",
    "\n",
    "### tensor_1 == image_data\n",
    "### tensor_2 == image_label\n",
    "\"\"\"\n",
    "A tf.data.Dataset represents a sequence of elements, in which each element contains one or more ---Tensor-- objects.\n",
    "For example, in an--- image pipeline, an element might be a ----single training example---, with a pair of tensors\n",
    "representing the image data and a label.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset.from_tensor_slices()\n",
    "### Dataset.batch()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more tf.Tensor objects.\n",
    "\n",
    "    Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more tf.data.Dataset objects.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### tf.data.Iterator\n",
    "\n",
    "\"\"\"\n",
    "A tf.data.Iterator provides the main way to extract elements from a dataset. \n",
    "The operation returned by Iterator.get_next() yields the next element of a Dataset when executed,\n",
    "and typically acts as the interface between input pipeline code and your model.\n",
    "\n",
    "The simplest iterator is a \"one-shot iterator\", which is associated with a particular Dataset and \n",
    "iterates through it once.\n",
    "\n",
    "For more sophisticated uses, the Iterator.initializer operation enables you to reinitialize \n",
    "and parameterize an iterator with different datasets, \n",
    "so that you can, for example, \n",
    "iterate over training and validation data multiple times in the same program.\n",
    "\"\"\"\n",
    "\n",
    "### Dataset structure\n",
    "# --- dataset >> elements >> tf.Tensor -- components >> tf.TensorShape\n",
    "\n",
    "\"\"\"\n",
    "Dataset structure\n",
    "\n",
    "A dataset comprises ---elements--- that each have the same structure. \n",
    "An element contains one or more ----tf.Tensor objects---, called ----components---.\n",
    "----- Each component has a tf.DType representing the type of elements in the tensor\n",
    "----- and a tf.TensorShape representing the (possibly partially specified) static shape of each element. \n",
    "\"\"\"\n",
    "\n",
    "### PROPERTIES ===>>  Dataset.output_types and Dataset.output_shapes\n",
    "\"\"\"\n",
    "The Dataset.output_types and Dataset.output_shapes properties \n",
    "\n",
    "----allow you to inspect the inferred types \n",
    "----and shapes of each component of a dataset element. \n",
    "\n",
    "The nested structure of these properties map to the structure of an element, \n",
    "--- which may be a single tensor, \n",
    "--- a tuple of tensors, \n",
    "--- or a nested tuple of tensors.\n",
    "\"\"\"\n",
    "\n",
    "### \n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(1000,)\n",
      "<TensorSliceDataset shapes: (1000,), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 1000]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "#\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "<TensorSliceDataset shapes: ((), (100,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "#\n",
    "print(dataset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is often convenient to give names to each component of an element, \n",
    "for example if they represent different features of a training example. \n",
    "\n",
    "In addition to tuples, you can use collections.namedtuple or a dictionary mapping strings to tensors \n",
    "to represent a single element of a Dataset.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.float32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([])}\n"
     ]
    }
   ],
   "source": [
    "### Official \n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': tf.int32, 'a': tf.int32}\n",
      "{'b': TensorShape([Dimension(100)]), 'a': TensorShape([Dimension(500)])}\n"
     ]
    }
   ],
   "source": [
    "## DHANKAR ---\n",
    "\n",
    "dataset_11 = tf.data.Dataset.from_tensor_slices(\n",
    "   {\n",
    "    \"a\": tf.random_uniform([4, 500], maxval=1000, dtype=tf.int32),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    }\n",
    "      )\n",
    "\n",
    "print(dataset_11.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset_11.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.data' has no attribute 'CsvDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31dd95f298fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m   \u001b[0;31m# Eight required float columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.data' has no attribute 'CsvDataset'"
     ]
    }
   ],
   "source": [
    "### CSV Uploads --ERROR --- FATT \n",
    "# latest version of TF == has the CSV Func \n",
    "## Documentation for version --- 1.12 \n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/data/CsvDataset\n",
    "\n",
    "#v-1.6.0 --- Has Experimental --\n",
    "##- tensorflow/tensorflow/python/data/experimental/benchmarks/csv_dataset_benchmark.py\n",
    "\n",
    "\n",
    "# Right now using - v-1.5.0 --- which does not . \n",
    "# /a6_18/OwnFork_TensorFlow/tensorflow/tensorflow/contrib/data/python/ops/readers.py\n",
    "\n",
    "\n",
    "# Creates a dataset that reads all of the records from two CSV files, each with\n",
    "# eight float columns\n",
    "filenames = [\"/media/dhankar/Dhankar_1/a6_18/Tensors_et_al/date_fmts.csv\"]\n",
    "\n",
    "record_defaults = [tf.float32] * 8   # Eight required float columns\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##FATT --- CSV OnHold\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "## https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/example/example.proto\n",
    "\n",
    "\"\"\"\n",
    "If your dataset consist of features, where each feature is a list of values of the same type, \n",
    "tf.train.Example is the right component to use.\n",
    "\n",
    "We have a number of features, \n",
    "each being a list where every entry has the same data type. \n",
    "In order to store these features in a TFRecord, \n",
    "we fist need to create the lists that constitute the features.\n",
    "\n",
    "tf.train.BytesList\n",
    "tf.train.FloatList\n",
    "tf.train.Int64List \n",
    "\n",
    "are at the core of a tf.train.Feature. \n",
    "\n",
    "All three have a single attribute value, which expects a list of respective \n",
    "--- bytes, \n",
    "--- float, \n",
    "--- int.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "### tf.train.Feature\n",
    "\"\"\"\n",
    "tf.train.Feature ---  wraps a list of data of a specific type so Tensorflow can understand it.\n",
    "It has a single attribute, which is a ---union of ----bytes_list/float_list/int64_list. \n",
    "Being a union, the stored list can be of type \n",
    "--- tf.train.BytesList (attribute name bytes_list), \n",
    "--- tf.train.FloatList (attribute name float_list), \n",
    "--- tf.train.Int64List (attribute name int64_list).\n",
    "\n",
    "tf.train.Features ----PLURAL----Features---- is a collection of named features. \n",
    "It has a single attribute feature that expects a dictionary where the --- key ----is the name of the features \n",
    "---- and the value a tf.train.Feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "In our example, each TFRecord represents the movie ratings and corresponding suggestions \n",
    "of a single user (a single sample). \n",
    "Writing recommendations for all users in the dataset follows the same process. \n",
    "It is important that the type of a feature (e.g. float for the movie rating) is the same across all samples \n",
    "in the dataset. \n",
    "This conformance criterion and others are defined in the protocol buffer definition of tf.train.Example.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Suggestion': 'Inception', 'Suggestion Purchased': 1.0, 'Movie': ['The Shawshank Redemption', 'Fight Club'], 'Purchase Price': 9.99, 'Age': 29, 'Movie Ratings': [9.0, 9.7]}\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "data = {\n",
    "    'Age': 29,\n",
    "    'Movie': ['The Shawshank Redemption', 'Fight Club'],\n",
    "    'Movie Ratings': [9.0, 9.7],\n",
    "    'Suggestion': 'Inception',\n",
    "    'Suggestion Purchased': 1.0,\n",
    "    'Purchase Price': 9.99\n",
    "}\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"Age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 29\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Movie\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"The Shawshank Redemption\"\n",
      "        value: \"Fight Club\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Movie Ratings\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 9.0\n",
      "        value: 9.699999809265137\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Purchase Price\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 9.989999771118164\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Suggestion\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"Inception\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Suggestion Purchased\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 1.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Example\n",
    "# Source --- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'Age': tf.train.Feature(\n",
    "        int64_list=tf.train.Int64List(value=[data['Age']])),\n",
    "    'Movie': tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[m.encode('utf-8') for m in data['Movie']])),\n",
    "    'Movie Ratings': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(value=data['Movie Ratings'])),\n",
    "    'Suggestion': tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[data['Suggestion'].encode('utf-8')])),\n",
    "    'Suggestion Purchased': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(\n",
    "            value=[data['Suggestion Purchased']])),\n",
    "    'Purchase Price': tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(value=[data['Purchase Price']]))\n",
    "}))\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write TFrecord file\n",
    "with tf.python_io.TFRecordWriter('customer_1.tfrecord') as writer:\n",
    "    #\n",
    "    writer.write(example.SerializeToString())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 29\n",
      "Purchase Price: 9.989999771118164\n",
      "Movie: SparseTensorValue(indices=array([[0],\n",
      "       [1]]), values=array([b'The Shawshank Redemption', b'Fight Club'], dtype=object), dense_shape=array([2]))\n",
      "Suggestion Purchased: 1.0\n",
      "Movie Ratings: SparseTensorValue(indices=array([[0],\n",
      "       [1]]), values=array([9. , 9.7], dtype=float32), dense_shape=array([2]))\n",
      "Suggestion: b'Inception'\n"
     ]
    }
   ],
   "source": [
    "# Read and print data:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['customer_1.tfrecord'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "# Define features\n",
    "read_features = {\n",
    "    'Age': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'Movie': tf.VarLenFeature(dtype=tf.string),\n",
    "    'Movie Ratings': tf.VarLenFeature(dtype=tf.float32),\n",
    "    'Suggestion': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'Suggestion Purchased': tf.FixedLenFeature([], dtype=tf.float32),\n",
    "    'Purchase Price': tf.FixedLenFeature([], dtype=tf.float32)}\n",
    "\n",
    "# Extract features from serialized data\n",
    "read_data = tf.parse_single_example(serialized=serialized_example,\n",
    "                                    features=read_features)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "# Print features\n",
    "for name, tensor in read_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': [{'Actors': ['Tim Robbins', 'Morgan Freeman'], 'Movie Rating': 9.0, 'Movie Name': 'The Shawshank Redemption'}, {'Actors': ['Brad Pitt', 'Edward Norton', 'Helena Bonham Carter'], 'Movie Rating': 9.7, 'Movie Name': 'Fight Club'}], 'Age': 19, 'Favorites': ['Majesty Rose', 'Savannah Outen', 'One Direction'], 'Locale': 'pt_BR'}\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "data1 = {\n",
    "    # Context\n",
    "    'Locale': 'pt_BR',\n",
    "    'Age': 19,\n",
    "    'Favorites': ['Majesty Rose', 'Savannah Outen', 'One Direction'],\n",
    "    # Data\n",
    "    'Data': [\n",
    "        {   # Movie 1\n",
    "            'Movie Name': 'The Shawshank Redemption',\n",
    "            'Movie Rating': 9.0,\n",
    "            'Actors': ['Tim Robbins', 'Morgan Freeman']\n",
    "        },\n",
    "        {   # Movie 2\n",
    "            'Movie Name': 'Fight Club',\n",
    "            'Movie Rating': 9.7,\n",
    "            'Actors': ['Brad Pitt', 'Edward Norton', 'Helena Bonham Carter']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context {\n",
      "  feature {\n",
      "    key: \"Age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 19\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Favorites\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"Majesty Rose\"\n",
      "        value: \"Savannah Outen\"\n",
      "        value: \"One Direction\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"Locale\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"pt_BR\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature_lists {\n",
      "  feature_list {\n",
      "    key: \"Movie Actors\"\n",
      "    value {\n",
      "      feature {\n",
      "        bytes_list {\n",
      "          value: \"Tim Robbins\"\n",
      "          value: \"Morgan Freeman\"\n",
      "        }\n",
      "      }\n",
      "      feature {\n",
      "        bytes_list {\n",
      "          value: \"Brad Pitt\"\n",
      "          value: \"Edward Norton\"\n",
      "          value: \"Helena Bonham Carter\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature_list {\n",
      "    key: \"Movie Names\"\n",
      "    value {\n",
      "      feature {\n",
      "        bytes_list {\n",
      "          value: \"The Shawshank Redemption\"\n",
      "        }\n",
      "      }\n",
      "      feature {\n",
      "        bytes_list {\n",
      "          value: \"Fight Club\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature_list {\n",
      "    key: \"Movie Ratings\"\n",
      "    value {\n",
      "      feature {\n",
      "        float_list {\n",
      "          value: 9.0\n",
      "        }\n",
      "      }\n",
      "      feature {\n",
      "        float_list {\n",
      "          value: 9.699999809265137\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the context features (short form)\n",
    "customer = tf.train.Features(feature={\n",
    "    'Locale': tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "        value=[data1['Locale'].encode('utf-8')])),\n",
    "    'Age': tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "        value=[data1['Age']])),\n",
    "    'Favorites': tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "        value=[m.encode('utf-8') for m in data1['Favorites']]))\n",
    "})\n",
    "\n",
    "# Create sequence data\n",
    "names_features = []\n",
    "ratings_features = []\n",
    "actors_features = []\n",
    "\n",
    "for movie in data1['Data']:\n",
    "    # Create each of the features, then add it to the\n",
    "    # corresponding feature list\n",
    "    movie_name_feature = tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[movie['Movie Name'].encode('utf-8')]))\n",
    "    names_features.append(movie_name_feature)\n",
    "    \n",
    "    movie_rating_feature = tf.train.Feature(\n",
    "        float_list=tf.train.FloatList(value=[movie['Movie Rating']]))\n",
    "    ratings_features.append(movie_rating_feature)\n",
    "                                             \n",
    "    movie_actors_feature = tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(\n",
    "            value=[m.encode('utf-8') for m in movie['Actors']]))\n",
    "    actors_features.append(movie_actors_feature)\n",
    "\n",
    "movie_names = tf.train.FeatureList(feature=names_features)\n",
    "movie_ratings = tf.train.FeatureList(feature=ratings_features)\n",
    "movie_actors = tf.train.FeatureList(feature=actors_features)\n",
    "\n",
    "movies = tf.train.FeatureLists(feature_list={\n",
    "    'Movie Names': movie_names,\n",
    "    'Movie Ratings': movie_ratings,\n",
    "    'Movie Actors': movie_actors\n",
    "})\n",
    "\n",
    "# Create the SequenceExample\n",
    "example = tf.train.SequenceExample(context=customer, feature_lists=movies)\n",
    "\n",
    "print(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write TFrecord file\n",
    "with tf.python_io.TFRecordWriter('customer_2.tfrecord') as writer:\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Name: , Context feature 'Locale' is required but could not be found.\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=2, Ncontext_sparse=1, Nfeature_list_dense=2, Nfeature_list_sparse=1, Tcontext_dense=[DT_INT64, DT_STRING], context_dense_shapes=[[], []], context_sparse_types=[DT_STRING], feature_list_dense_shapes=[[], []], feature_list_dense_types=[DT_STRING, DT_FLOAT], feature_list_sparse_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReaderReadV2:1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/Const_1, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]\n\nCaused by op 'ParseSingleSequenceExample/ParseSingleSequenceExample', defined at:\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-6aaa17de5d93>\", line 26, in <module>\n    sequence_features=sequence_features)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 944, in parse_single_sequence_example\n    feature_list_dense_defaults, example_name, name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 1141, in _parse_single_sequence_example_raw\n    name=name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 456, in _parse_single_sequence_example\n    feature_list_dense_shapes=feature_list_dense_shapes, name=name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Name: , Context feature 'Locale' is required but could not be found.\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=2, Ncontext_sparse=1, Nfeature_list_dense=2, Nfeature_list_sparse=1, Tcontext_dense=[DT_INT64, DT_STRING], context_dense_shapes=[[], []], context_sparse_types=[DT_STRING], feature_list_dense_shapes=[[], []], feature_list_dense_types=[DT_STRING, DT_FLOAT], feature_list_sparse_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReaderReadV2:1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/Const_1, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Name: , Context feature 'Locale' is required but could not be found.\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=2, Ncontext_sparse=1, Nfeature_list_dense=2, Nfeature_list_sparse=1, Tcontext_dense=[DT_INT64, DT_STRING], context_dense_shapes=[[], []], context_sparse_types=[DT_STRING], feature_list_dense_shapes=[[], []], feature_list_dense_types=[DT_STRING, DT_FLOAT], feature_list_sparse_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReaderReadV2:1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/Const_1, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6aaa17de5d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Context:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nData'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \"\"\"\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4756\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4757\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4758\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Name: , Context feature 'Locale' is required but could not be found.\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=2, Ncontext_sparse=1, Nfeature_list_dense=2, Nfeature_list_sparse=1, Tcontext_dense=[DT_INT64, DT_STRING], context_dense_shapes=[[], []], context_sparse_types=[DT_STRING], feature_list_dense_shapes=[[], []], feature_list_dense_types=[DT_STRING, DT_FLOAT], feature_list_sparse_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReaderReadV2:1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/Const_1, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]\n\nCaused by op 'ParseSingleSequenceExample/ParseSingleSequenceExample', defined at:\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-6aaa17de5d93>\", line 26, in <module>\n    sequence_features=sequence_features)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 944, in parse_single_sequence_example\n    feature_list_dense_defaults, example_name, name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\", line 1141, in _parse_single_sequence_example_raw\n    name=name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 456, in _parse_single_sequence_example\n    feature_list_dense_shapes=feature_list_dense_shapes, name=name)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/dhankar/anaconda2/envs/dc_info_venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Name: , Context feature 'Locale' is required but could not be found.\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=2, Ncontext_sparse=1, Nfeature_list_dense=2, Nfeature_list_sparse=1, Tcontext_dense=[DT_INT64, DT_STRING], context_dense_shapes=[[], []], context_sparse_types=[DT_STRING], feature_list_dense_shapes=[[], []], feature_list_dense_types=[DT_STRING, DT_FLOAT], feature_list_sparse_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReaderReadV2:1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_sparse_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/Const_1, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]\n"
     ]
    }
   ],
   "source": [
    "# Read and print data:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['customer_1.tfrecord'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "# Define features\n",
    "context_features = {\n",
    "    'Locale': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'Age': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'Favorites': tf.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "sequence_features = {\n",
    "    'Movie Names': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "    'Movie Ratings': tf.FixedLenSequenceFeature([], dtype=tf.float32),\n",
    "    'Movie Actors': tf.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "\n",
    "# Extract features from serialized data\n",
    "context_data, sequence_data = tf.parse_single_sequence_example(\n",
    "    serialized=serialized_example,\n",
    "    context_features=context_features,\n",
    "    sequence_features=sequence_features)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "# Print features\n",
    "print('Context:')\n",
    "for name, tensor in context_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))\n",
    "\n",
    "print('\\nData')\n",
    "for name, tensor in sequence_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Age: 19\n",
      "Favorites: SparseTensorValue(indices=array([[0],\n",
      "       [1],\n",
      "       [2]]), values=array([b'Majesty Rose', b'Savannah Outen', b'One Direction'], dtype=object), dense_shape=array([3]))\n",
      "Locale: b'pt_BR'\n",
      "\n",
      "Data\n",
      "Movie Actors: SparseTensorValue(indices=array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [1, 2]]), values=array([b'Tim Robbins', b'Morgan Freeman', b'Brad Pitt', b'Edward Norton',\n",
      "       b'Helena Bonham Carter'], dtype=object), dense_shape=array([2, 3]))\n",
      "Movie Names: [b'The Shawshank Redemption' b'Fight Club']\n",
      "Movie Ratings: [9.  9.7]\n"
     ]
    }
   ],
   "source": [
    "#### DHANKAR ---- customer_2.tfrecord\n",
    "#\n",
    "\n",
    "# Read and print data:\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read TFRecord file\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['customer_2.tfrecord'])\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "# Define features\n",
    "context_features = {\n",
    "    'Locale': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    'Age': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'Favorites': tf.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "sequence_features = {\n",
    "    'Movie Names': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "    'Movie Ratings': tf.FixedLenSequenceFeature([], dtype=tf.float32),\n",
    "    'Movie Actors': tf.VarLenFeature(dtype=tf.string)\n",
    "}\n",
    "\n",
    "# Extract features from serialized data\n",
    "context_data, sequence_data = tf.parse_single_sequence_example(\n",
    "    serialized=serialized_example,\n",
    "    context_features=context_features,\n",
    "    sequence_features=sequence_features)\n",
    "\n",
    "# Many tf.train functions use tf.train.QueueRunner,\n",
    "# so we need to start it before we read\n",
    "tf.train.start_queue_runners(sess)\n",
    "\n",
    "# Print features\n",
    "print('Context:')\n",
    "for name, tensor in context_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))\n",
    "\n",
    "print('\\nData')\n",
    "for name, tensor in sequence_data.items():\n",
    "    print('{}: {}'.format(name, tensor.eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dc_info_venv]",
   "language": "python",
   "name": "conda-env-dc_info_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
